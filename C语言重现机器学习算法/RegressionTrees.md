## RegressionTrees

决策树实际上是将空间用超平面进行划分的一种方法，每次分割的时候，都将当前的空间一分为二， 这样使得每一个叶子节点都是在空间中的一个不相交的区域，在进行决策的时候，会根据输入样本每一维feature的值，一步一步往下，最后使得样本落入N个区域中的一个（假设有N个叶子节点）。

既然是决策树，那么必然会存在以下两个核心问题：如何选择划分点？如何决定叶节点的输出值？

一个回归树对应着输入空间（即特征空间）的一个划分以及在划分单元上的输出值。分类树中，我们采用信息论中的方法，通过计算选择最佳划分点。而在回归树中，采用的是启发式的方法。假如我们有n个特征，每个特征有si(i∈(1,n))si(i∈(1,n))个取值，那我们遍历所有特征，尝试该特征所有取值，对空间进行划分，直到取到特征j的取值s，使得损失函数最小，这样就得到了一个划分点。描述该过程的公式如下：min(js)[min(c1)Loss(yi,c1)+min(c2)Loss(yi,c2)]

假设将输入空间划分为M个单元：*R*1,*R*2,...,*R**m*R1,R2,...,Rm 那么每个区域的输出值就是：*c**m*=*a**v**e*(*y**i*|*x**i*∈*R**m*)cm=ave(yi|xi∈Rm)也就是该区域内所有点y值的平均数。

### Instance

为了便于理解，下面举一个简单实例。训练数据见下表，目标是得到一棵最小二乘回归树。

| x    | 1    | 2    | 3    | 4    | 5    | 6    | 7    | 8    | 9    | 10   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| y    | 5.56 | 5.7  | 5.91 | 6.4  | 6.8  | 7.05 | 8.9  | 8.7  | 9    | 9.05 |

1. 选择最优切分变量j与最优切分点s

在本数据集中，只有一个变量，因此最优切分变量自然是x。

接下来我们考虑9个切分点[1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5][1.5,2.5,3.5,4.5,5.5,6.5,7.5,8.5,9.5]你可能会问，为什么会带小数点呢？类比于篮球比赛的博彩，倘若两队比分是96:95，而盘口是“让1分 A队胜B队”，那A队让1分之后，到底是A队赢还是B队赢了？所以我们经常可以看到“让0.5分 A队胜B队”这样的盘口。在这个实例中，也是这个道理。

损失函数定义为平方损失函数 Loss(y,f(x))=(f(x)−y)2Loss(y,f(x))=(f(x)−y)2，将上述9个切分点一依此代入下面的公式，其中 cm=ave(yi|xi∈Rm)cm=ave(yi|xi∈Rm) 
min(js)[min(c1)Loss(yi,c1)+min(c2)Loss(yi,c2)]

例如，取 s=1.5s=1.5。此时 R1={1},R2={2,3,4,5,6,7,8,9,10}R1={1},R2={2,3,4,5,6,7,8,9,10}，这两个区域的输出值分别为： 
c1=5.56,c2=19(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05)=7.50c1=5.56,c2=19(5.7+5.91+6.4+6.8+7.05+8.9+8.7+9+9.05)=7.50。得到下表：

| s    | 1.5  | 2.5  | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  | 8.5  | 9.5  |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| c1   | 5.56 | 5.63 | 5.72 | 5.89 | 6.07 | 6.24 | 6.62 | 6.88 | 7.11 |
| c2   | 7.5  | 7.73 | 7.99 | 8.52 | 8.54 | 8.91 | 8.92 | 9.03 | 9.05 |

把*c*1,*c*2c1,c2的值代入到上式，如：*m*(1.5)=0+15.72=15.72m(1.5)=0+15.72=15.72。同理，可获得下表：

| s    | 1.5   | 2.5   | 3.5  | 4.5  | 5.5  | 6.5  | 7.5  | 8.5   | 9.5   |
| ---- | ----- | ----- | ---- | ---- | ---- | ---- | ---- | ----- | ----- |
| m(s) | 15.72 | 12.07 | 8.36 | 5.78 | 3.91 | 1.93 | 8.01 | 11.73 | 15.74 |

显然取 *s*=6.5s=6.5时，*m*(*s*)m(s)最小。因此，第一个划分变量*j*=*x*,*s*=6.5

用选定的(j,s)划分区域，并决定输出值

两个区域分别是：*R*1={1,2,3,4,5,6},*R*2={7,8,9,10}R1={1,2,3,4,5,6},R2={7,8,9,10}输出值*c**m*=*a**v**e*(*y**i*|*x**i*∈*R**m*),*c*1=6.24,*c*2=8.91

对两个子区域继续调用步骤1、步骤2

对*R*1R1继续进行划分

计算m(s)：

s=3.5时m(s)最小。

生成回归树

**假设**在生成**3个区域**之后停止划分，那么最终生成的回归树形式如下：

T = 5.72           x<3.5

​	6.75           3.5<=x<6.5

​	 8.91         6.5<x

