一、Logistics分布

定义：X是连续随机变量，X服从logistic分布，则X具有下列的分布函数和密度函数：![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2288533678,3601271205&fm=173&app=49&f=JPEG?w=256&h=107&s=5AA83C6287B979805AFD14CB0000E0B1)

其中，μ为位置参数，γ为形状参数![img](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=1634107049,3384705189&fm=173&app=49&f=JPEG?w=554&h=210&s=5AAA3C628D686D0158DDA0DA0000C0B1)

曲线在中心附近增长速度较快，并且γ值越小，曲线在中心附近的增长速度越快。

特别的，当μ=0，γ=1的时候就是sigmoid函数。

### 二、二项Logistics回归原理

二项Logistic回归模型时一种分类模型，由条件概率分布P(Y|X)表示，随机变量Y取0或1。

定义二项logistic回归模型的条件分布如下：![img](https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=206071143,2563302009&fm=173&app=49&f=JPEG?w=183&h=99&s=7AAE3462935149C80AFDF1CB0000E0B1)

其中x∈Rn是输入，Y∈{0,1}是输出，W∈Rn和b∈R是参数，w称为权重，b称为偏置。

有时为了方便会将权重向量和输入向量进行扩充：

w = (w1,w2, …, wn, b)T，x = (x1,x2, …, xn, 1)T

所以，logistic回归模型变为：![img](https://ss0.baidu.com/6ONWsjip0QIZ8tyhnq/it/u=206071143,2563302009&fm=173&app=49&f=JPEG?w=183&h=99&s=7AAE3462935149C80AFDF1CB0000E0B1)

得到概率之后，我们可以通过设定一个阈值将样本分成两类。如：阈值为0.5的时候，当大于0.5则为一类，小于0.5为另一类。

### 三、参数估计

有了以上的模型，我们就需要对模型中的参数w求出来。我们可以使用极大似然估计法估计模型的参数。

设：![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=158536153,1119486048&fm=173&app=49&f=JPEG?w=354&h=38)

似然函数为：![img](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=2369190773,3264343860&fm=173&app=49&f=JPEG?w=372&h=119&s=0AAA7A22CDB4CE110A71ECD2000080B1)

对似然函数：![img](https://ss2.baidu.com/6ONYsjip0QIZ8tyhnq/it/u=2789067617,3040887077&fm=173&app=49&f=JPEG?w=554&h=240&s=1AAA72238B584CC80E5DC4DE0000C0B1)

对L(w)求极大值，得到w的估计值。通常采用梯度下降法或拟牛顿法求解参数w。

### 四、Logistic回归正则化

正则化是为了解决过拟合问题。分为L1和L2正则化。目标函数中加入正则化，即加入模型复杂性的评估。正则化符合奥卡姆剃刀原理，即：在所有可能的模型中，能够很好的解释已知数据并且十分简单的模型才是最好的模型。

加入正则化后，模型的目标函数变为：![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=65965180,1613878227&fm=173&app=49&f=JPEG?w=186&h=52&s=5AAC3C62C536C8310C75D0C60000C0B1)

P表示范数，p=1为L1正则化，p=2为L2正则化 

L1正则化：向量中各元素绝对值的和。关键在于能够对特征进行自动选择，稀疏参数可以减少非必要的特征引入噪声。

L2正则化：向量中个元素的平方和，L2会使得各元素尽可能小，但都不为零![img](https://ss1.baidu.com/6ONXsjip0QIZ8tyhnq/it/u=2545292521,2469499767&fm=173&app=49&f=JPEG?w=554&h=269&s=8EC0E912B39359DA126D09C80300C0B3)

左边为L1正则化，右边为L2正则化。假设权重参数w只有二维w1和w2。L1为各元素绝对值和，即|w1|+|w2| = C，则得到的形状为棱形，L2为(w1)^2+(w2)^2 = C，则形状为圆。很容易可以发现L1更容易在顶点处相切，L2则不容易在顶点处相切。顶点处则其中一个参数为0，这就是为什么L1会使得参数稀疏的原因。

### Logistic回归和线性回归区别

1. Logistic回归在线性回归的实数输出范围加上sigmoid函数，将输出值收敛在0~1之间。其目标函数也因此从差平方和函数变为对数损失函数。
2. 逻辑回归和线性回归都是广义的线性回归，线性回归是使用最小二乘法优化目标函数，而逻辑回归是使用梯度下降或者拟牛顿法。
3. 线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围需要在[0,1]。逻辑回归是一种减少预测范围，将预测值限定为[0,1]间的一种回归模型。因而对于二分类问题，逻辑回归的鲁棒性更好。
4. 逻辑回归是以线性回归为理论支持的，但线性回归模型无法做到sigmoid的非线性形式。Sigmoid可以轻松处理0/1分类问题。



# 2.逻辑回归的推导

### 　　1.Sigmoid 函数：（z值就是预测值）

　　　　Logistic Regression虽然名字里带“回归”，但是它实际上是一种分类方法，用于两分类问题（即输出只有两种）。根据第二章中的步骤，需要先找到一个预测函数（*h*），显然，该函数的输出必须是两个值（分别代表两个类别）

　　　　所以利用了Logistic函数（或称为Sigmoid函数），函数形式为：

![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180915212002417-2088253873.png)



　　　二分类问题的概率与自变量之间的关系图形往往是一个S型曲线，如图所示，采用的Sigmoid函数实现：

　　　![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180915212103665-1254793184.png)![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180915212226372-1688005719.png)

### 　　2.推导![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180915224152547-1397909853.png)

### 　　3.得到似然函数，和对数似：![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180916000556943-180657552.png)

### 　　4.求导计算![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180916000718795-1369583682.png)

　　5.求解(通过参数的变化得到最优解)：

　　![img](https://img2018.cnblogs.com/blog/1360469/201809/1360469-20180916003204249-1444639082.png)





## 3种方法实现逻辑回归多分类

## **One-Vs-All**

假设我们要解决一个分类问题，该分类问题有三个类别，分别用△，□和×表示，每个实例（Entity）有两个属性（Attribute），如果把属性 1 作为 X 轴，属性 2 作为 Y 轴，训练集（Training Dataset）的分布可以表示为下图：

![img](https://pic4.zhimg.com/80/v2-db3b37d71742a747799ebc1ac19b0493_1440w.jpg)

One-Vs-All（或者叫 One-Vs-Rest）的思想是把一个多分类的问题变成多个二分类的问题。转变的思路就如同方法名称描述的那样，选择其中一个类别为正类（Positive），使其他所有类别为负类（Negative）。比如第一步，我们可以将三角形所代表的实例全部视为正类，其他实例全部视为负类，得到的分类器如图：

![img](https://pic4.zhimg.com/80/v2-041fe01b69b59c723a5f1184d0cd715b_1440w.jpg)

同理我们把 X 视为正类，其他视为负类，可以得到第二个分类器：

![img](https://pic3.zhimg.com/80/v2-40e01ebb96ffbb0f9ff2e9353661e756_1440w.jpg)

最后，第三个分类器是把正方形视为正类，其余视为负类：

![img](https://pic3.zhimg.com/80/v2-780ebb775fb3270e50d704dd7d9eea72_1440w.jpg)

对于一个三分类问题，我们最终得到 3 个二元分类器。在预测阶段，每个分类器可以根据测试样本，得到当前正类的概率。即 P(y = i | x; θ)，i = 1, 2, 3。选择计算结果最高的分类器，其正类就可以作为预测结果。

One-Vs-All 最为一种常用的二分类拓展方法，其优缺点也十分明显。

优点：普适性还比较广，可以应用于能输出值或者概率的分类器，同时效率相对较好，有多少个类别就训练多少个分类器。

缺点：很容易造成训练集样本数量的不平衡（Unbalance），尤其在类别较多的情况下，经常容易出现正类样本的数量远远不及负类样本的数量，这样就会造成分类器的偏向性。

------

## **One-Vs-One**

相比于 One-Vs-All 由于样本数量可能的偏向性带来的不稳定性，One-Vs-One 是一种相对稳健的扩展方法。对于同样的三分类问题，我们像举行车轮作战一样让不同类别的数据两两组合训练分类器，可以得到 3 个二元分类器。

它们分别是三角形与 x 训练得出的分类器：

![img](https://pic3.zhimg.com/80/v2-9f115cac662cdeea69d7a0142d16035a_1440w.jpg)

三角形与正方形训练的出的分类器：

![img](https://pic3.zhimg.com/80/v2-9faaa0380fd1a058b5fb9e2f1c18fa0e_1440w.jpg)

以及正方形与 x 训练得出的分类器：

![img](https://pic1.zhimg.com/80/v2-bb92df28ff9592c2c602d5f23272a1c8_1440w.jpg)

假如我们要预测的一个数据在图中红色圆圈的位置，那么第一个分类器会认为它是 x，第二个分类器会认为它偏向三角形，第三个分类器会认为它是 x，经过三个分类器的投票之后，可以预测红色圆圈所代表的数据的类别为 x。

![img](https://pic2.zhimg.com/80/v2-bb2e27c1b5c6edaf20bd3d5411133a51_1440w.jpg)

任何一个测试样本都可以通过分类器的投票选举出预测结果，这就是 One-Vs-One 的运行方式。

当然这一方法也有显著的优缺点，其缺点是训练出更多的 Classifier，会影响预测时间。

虽然在本文的例子中，One-Vs-All 和 One-Vs-One 都得到三个分类器，但实际上仔细思考就会发现，如果有 k 个不同的类别，对于 One-Vs-All 来说，一共只需要训练 k 个分类器，而 One-Vs-One 则需训练 C(k, 2) 个分类器，只是因为在本例种，k = 3 时恰好两个值相等，一旦 k 值增多，One-Vs-One 需要训练的分类器数量会大大增多。

当然 One-Vs-One 的优点也很明显，它在一定程度上规避了数据集 unbalance 的情况，性能相对稳定，并且需要训练的模型数虽然增多，但是每次训练时训练集的数量都降低很多，其训练效率会提高。

------

## **Softmax**

在二元的逻辑回归模型中，我们用 Sigmoid 函数将一个多维数据（一个样本）映射到一个 0 - 1 之间的数值上，有没有什么方法从数学上让一个样本映射到多个 0 - 1 之间的数值呢？答案是通过 Softmax 函数。

![img](https://pic1.zhimg.com/80/v2-1d4e46217f6df995854b01229434f98c_1440w.jpg)

![img](https://pic2.zhimg.com/80/v2-231ed6aa6d3f5ab5cac753fd7e710341_1440w.jpg)

 使所有概率之和为 1，是对概率分布进行归一化。

为什么选用指数函数呢？有一些简单的理由 [2]：

1. 指数函数简单，并且是非线性的
2. 该函数严格递增
3. 这是一个凸函数

定义了新的假设函数（hypothesis function）之后，我们要得到其对应的代价函数（cost function）。

![img](https://pic3.zhimg.com/80/v2-ab690c0473c9056d1024316661bd89da_1440w.jpg)



其中 ![[公式]](https://www.zhihu.com/equation?tex=1%5Cleft%5C%7B+%5Ccdot+%5Cright%5C%7D) 的取值规则为大括号内的表达式值为真时，取 1，为假时取 0。

对该代价函数求最优解同样可以使用如梯度下降之类的迭代算法，其梯度公式如下：



![img](https://pic3.zhimg.com/80/v2-0844408e1498b558fa8b3edd4ed24326_1440w.jpg)



有了偏导数，就可以对代价函数进行优化，最终求解。

本质上讲，Softmax 回归就是 logistic 回归进行多分类时的一种数学拓展，如果让分类数为 2 带入 softmax 回归，会发现其本质上和 logistic 回归是一样的。

在处理一些样本可能丛属多个类别的分类问题是，使用 one vs one 或 one vs all 有可能达到更好的效果。

Softmax 回归适合处理一个样本尽可能属于一种类别的多分类问题。



